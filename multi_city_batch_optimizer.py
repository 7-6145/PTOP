"""
å¤šåŸå¸‚æ‰¹é‡ä¼˜åŒ–ç³»ç»Ÿ - UltraThinkç‰ˆæœ¬
åŸºäºfinal_optimizer_with_outputs.pyçš„ç®—æ³•ï¼Œæ”¯æŒæ‰¹é‡å¤„ç†åå‡ ä¸ªåŸå¸‚çš„æ•°æ®
åŒ…å«æ™ºèƒ½åŸå¸‚è¯†åˆ«ã€æ•°æ®é¢„å¤„ç†ã€æ‰¹é‡ä¼˜åŒ–å’Œç»“æœæ±‡æ€»åŠŸèƒ½
"""

import numpy as np
import pandas as pd
from numba import jit, prange
import logging
import time
from typing import Dict, Any, Tuple, List
import matplotlib.pyplot as plt
from pathlib import Path
from datetime import datetime
import json
import re
import os
import glob
from concurrent.futures import ProcessPoolExecutor
import traceback

try:
    import geopandas as gpd
    HAS_GEOPANDAS = True
except ImportError:
    HAS_GEOPANDAS = False
    print("Warning: GeoPandas not available, will create CSV files instead")

from data_preprocessing import DataProcessor
from acceleration_utils import fast_population_coverage

# è®¾ç½®ä¸­æ–‡å­—ä½“å’Œæ—¥å¿—
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei']
plt.rcParams['axes.unicode_minus'] = False
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# å¤ç”¨final_optimizer_with_outputs.pyçš„æ ¸å¿ƒç®—æ³•å‡½æ•°
@jit(nopython=True, cache=True)
def intelligent_initialization(original_positions: np.ndarray,
                             population_size: int,
                             max_move_ratio: float = 0.15,
                             max_move_distance: float = 0.003) -> np.ndarray:
    """æ™ºèƒ½åˆå§‹åŒ–ï¼šç²¾ç¡®æ§åˆ¶ç§»åŠ¨ç­–ç•¥"""
    n_stops = original_positions.shape[0]
    population = np.zeros((population_size, n_stops, 2))
    
    for i in range(population_size):
        population[i] = original_positions.copy()
        
        # æ¯ä¸ªä¸ªä½“åªç§»åŠ¨10-20%çš„ç«™ç‚¹
        n_move = max(1, int(n_stops * (0.1 + np.random.random() * max_move_ratio)))
        move_indices = np.random.choice(n_stops, n_move, replace=False)
        
        for idx in move_indices:
            # æ¸è¿›å¼ç§»åŠ¨ï¼šè·ç¦»æŒ‰æ­£æ€åˆ†å¸ƒ
            move_distance = np.abs(np.random.normal(0, max_move_distance * 0.3))
            move_distance = min(move_distance, max_move_distance)
            
            # éšæœºæ–¹å‘
            angle = np.random.random() * 2 * np.pi
            dx = move_distance * np.cos(angle)
            dy = move_distance * np.sin(angle)
            
            population[i, idx, 0] += dx
            population[i, idx, 1] += dy
    
    return population

@jit(nopython=True, cache=True)
def stability_aware_fitness(positions: np.ndarray,
                          original_positions: np.ndarray,
                          pop_points: np.ndarray,
                          pop_weights: np.ndarray,
                          coverage_radius: float) -> float:
    """ç¨³å®šæ€§æ„ŸçŸ¥é€‚åº”åº¦å‡½æ•°"""
    n_stops = positions.shape[0]
    
    # 1. è¦†ç›–ç‡è®¡ç®—
    coverage_rate = fast_population_coverage(
        positions, pop_points, pop_weights, coverage_radius
    )
    
    # 2. ç¨³å®šæ€§è®¡ç®—
    unmoved_count = 0
    total_movement = 0.0
    movement_penalty = 0.0
    
    for i in range(n_stops):
        dx = positions[i, 0] - original_positions[i, 0]
        dy = positions[i, 1] - original_positions[i, 1]
        movement = np.sqrt(dx * dx + dy * dy)
        
        total_movement += movement
        
        # ç¨³å®šæ€§ï¼šç§»åŠ¨å°äºé˜ˆå€¼è®¤ä¸ºæœªç§»åŠ¨
        if movement < 0.0001:  # ~11ç±³
            unmoved_count += 1
        else:
            # ç§»åŠ¨æƒ©ç½šï¼šè¶…å‡ºåˆç†èŒƒå›´å¤§å¹…æƒ©ç½š
            if movement > 0.005:  # ~550ç±³
                movement_penalty += movement * 10
            else:
                movement_penalty += movement
    
    stability_score = unmoved_count / n_stops
    avg_movement = total_movement / n_stops
    
    # ç»¼åˆé€‚åº”åº¦ï¼šä¼˜å…ˆè¦†ç›–ç‡ï¼Œå…¼é¡¾ç¨³å®šæ€§
    fitness = (
        coverage_rate * 10.0 +           # è¦†ç›–ç‡æƒé‡æœ€é«˜
        stability_score * 5.0 -          # ç¨³å®šæ€§å¥–åŠ±
        movement_penalty * 2.0           # ç§»åŠ¨æƒ©ç½š
    )
    
    return fitness

@jit(nopython=True, cache=True)
def adaptive_genetic_algorithm(original_positions: np.ndarray,
                             pop_points: np.ndarray,
                             pop_weights: np.ndarray,
                             coverage_radius: float,
                             population_size: int = 50,
                             max_generations: int = 100) -> Tuple[np.ndarray, float]:
    """è‡ªé€‚åº”é—ä¼ ç®—æ³•"""
    
    # æ™ºèƒ½åˆå§‹åŒ–
    population = intelligent_initialization(original_positions, population_size)
    
    best_individual = population[0].copy()
    best_fitness = stability_aware_fitness(
        best_individual, original_positions, pop_points, pop_weights, coverage_radius
    )
    
    stagnation_count = 0
    
    for generation in range(max_generations):
        # è®¡ç®—é€‚åº”åº¦
        fitness_scores = np.zeros(population_size)
        
        for i in range(population_size):
            fitness_scores[i] = stability_aware_fitness(
                population[i], original_positions, pop_points, pop_weights, coverage_radius
            )
            
            if fitness_scores[i] > best_fitness:
                best_fitness = fitness_scores[i]
                best_individual = population[i].copy()
                stagnation_count = 0
            else:
                stagnation_count += 1
        
        # æ—©åœç­–ç•¥
        if stagnation_count > 20:
            break
        
        # é€‰æ‹©æ’åº
        sorted_indices = np.argsort(fitness_scores)[::-1]
        
        # ç²¾è‹±ä¿ç•™
        elite_size = max(2, population_size // 10)
        new_population = np.zeros_like(population)
        
        for i in range(elite_size):
            new_population[i] = population[sorted_indices[i]].copy()
        
        # ç”Ÿæˆæ–°ä¸ªä½“
        for i in range(elite_size, population_size):
            # é”¦æ ‡èµ›é€‰æ‹©
            parent1_idx = sorted_indices[np.random.randint(0, min(5, population_size))]
            parent2_idx = sorted_indices[np.random.randint(0, min(5, population_size))]
            
            # ä¿å®ˆäº¤å‰
            child = population[parent1_idx].copy()
            
            # åªå¯¹10%çš„ç«™ç‚¹è¿›è¡Œäº¤å‰
            n_crossover = max(1, int(original_positions.shape[0] * 0.1))
            crossover_indices = np.random.choice(
                original_positions.shape[0], n_crossover, replace=False
            )
            
            for idx in crossover_indices:
                if np.random.random() < 0.5:
                    child[idx] = population[parent2_idx, idx].copy()
            
            # ä¿å®ˆå˜å¼‚
            if np.random.random() < 0.3:
                n_mutate = max(1, int(original_positions.shape[0] * 0.05))
                mutate_indices = np.random.choice(
                    original_positions.shape[0], n_mutate, replace=False
                )
                
                for idx in mutate_indices:
                    dx = np.random.normal(0, 0.001)
                    dy = np.random.normal(0, 0.001)
                    child[idx, 0] += dx
                    child[idx, 1] += dy
            
            new_population[i] = child
        
        population = new_population
    
    return best_individual, best_fitness

class CityDataMatcher:
    """æ™ºèƒ½åŸå¸‚æ•°æ®åŒ¹é…å™¨"""
    
    def __init__(self, bus_stops_dir: str, population_dir: str):
        """åˆå§‹åŒ–"""
        self.bus_stops_dir = Path(bus_stops_dir)
        self.population_dir = Path(population_dir)
        
    def extract_city_name(self, filename: str) -> str:
        """ä»æ–‡ä»¶åä¸­æå–åŸå¸‚åç§°"""
        # ç§»é™¤æ–‡ä»¶æ‰©å±•å
        base_name = Path(filename).stem
        
        # å¸¸è§çš„åŸå¸‚åæå–æ¨¡å¼
        patterns = [
            r'(\d{4})([^._]+)',  # åŒ¹é…å¦‚ "0577æ¸©å·"
            r'([^._\d]+)(?:_.*)?',  # åŒ¹é…å¦‚ "æ¸©å·_population_grid"
            r'([^._]+)',  # é€šç”¨æ¨¡å¼
        ]
        
        for pattern in patterns:
            match = re.search(pattern, base_name)
            if match:
                city_name = match.group(1) if len(match.groups()) == 1 else match.group(2)
                # æ¸…ç†åŸå¸‚å
                city_name = re.sub(r'[^\u4e00-\u9fff\w]', '', city_name)  # ä¿ç•™ä¸­æ–‡å’Œå­—æ¯æ•°å­—
                if len(city_name) >= 2:  # åŸå¸‚åè‡³å°‘2ä¸ªå­—ç¬¦
                    return city_name
        
        return base_name
    
    def discover_cities(self) -> List[Dict[str, Any]]:
        """è‡ªåŠ¨å‘ç°å¹¶åŒ¹é…åŸå¸‚æ•°æ®"""
        logger.info("ğŸ” å¼€å§‹è‡ªåŠ¨å‘ç°åŸå¸‚æ•°æ®...")
        
        # 1. å‘ç°å…¬äº¤ç«™ç‚¹æ–‡ä»¶
        bus_files = []
        for ext in ['*.shp', '*.csv']:
            bus_files.extend(list(self.bus_stops_dir.glob(ext)))
        
        # 2. å‘ç°äººå£æ•°æ®æ–‡ä»¶  
        pop_files = []
        for ext in ['*.csv', '*.shp']:
            pop_files.extend(list(self.population_dir.glob(ext)))
        
        logger.info(f"ğŸ“ å‘ç° {len(bus_files)} ä¸ªå…¬äº¤æ–‡ä»¶ï¼Œ{len(pop_files)} ä¸ªäººå£æ–‡ä»¶")
        
        # 3. æå–åŸå¸‚åå¹¶åŒ¹é…
        bus_cities = {}
        for file in bus_files:
            city = self.extract_city_name(file.name)
            if city:
                bus_cities[city] = file
        
        pop_cities = {}
        for file in pop_files:
            city = self.extract_city_name(file.name)
            if city:
                pop_cities[city] = file
        
        # 4. æ™ºèƒ½åŒ¹é…
        matched_cities = []
        
        for bus_city, bus_file in bus_cities.items():
            # å¯»æ‰¾æœ€ä½³åŒ¹é…çš„äººå£æ–‡ä»¶
            best_match = None
            best_score = 0
            
            for pop_city, pop_file in pop_cities.items():
                # è®¡ç®—åŒ¹é…åˆ†æ•°
                score = self._calculate_match_score(bus_city, pop_city)
                if score > best_score and score > 0.3:  # æœ€ä½åŒ¹é…é˜ˆå€¼
                    best_score = score
                    best_match = (pop_city, pop_file)
            
            if best_match:
                matched_cities.append({
                    'city_name': bus_city,
                    'bus_file': str(bus_file),
                    'population_file': str(best_match[1]),
                    'match_score': best_score
                })
                logger.info(f"âœ… åŒ¹é…æˆåŠŸ: {bus_city} (åŒ¹é…åº¦: {best_score:.2f})")
                logger.info(f"   å…¬äº¤: {bus_file.name}")
                logger.info(f"   äººå£: {best_match[1].name}")
            else:
                logger.warning(f"âŒ æœªæ‰¾åˆ°åŒ¹é…: {bus_city} - {bus_file.name}")
        
        logger.info(f"ğŸ¯ æˆåŠŸåŒ¹é… {len(matched_cities)} ä¸ªåŸå¸‚")
        return matched_cities
    
    def _calculate_match_score(self, name1: str, name2: str) -> float:
        """è®¡ç®—ä¸¤ä¸ªåŸå¸‚åçš„åŒ¹é…åˆ†æ•°"""
        # å®Œå…¨åŒ¹é…
        if name1 == name2:
            return 1.0
        
        # åŒ…å«å…³ç³»
        if name1 in name2 or name2 in name1:
            return 0.8
        
        # ç¼–è¾‘è·ç¦»åŒ¹é…
        distance = self._edit_distance(name1, name2)
        max_len = max(len(name1), len(name2))
        if max_len > 0:
            similarity = 1.0 - distance / max_len
            return max(0.0, similarity)
        
        return 0.0
    
    def _edit_distance(self, s1: str, s2: str) -> int:
        """è®¡ç®—ç¼–è¾‘è·ç¦»"""
        if len(s1) < len(s2):
            return self._edit_distance(s2, s1)
        
        if len(s2) == 0:
            return len(s1)
        
        previous_row = list(range(len(s2) + 1))
        for i, c1 in enumerate(s1):
            current_row = [i + 1]
            for j, c2 in enumerate(s2):
                insertions = previous_row[j + 1] + 1
                deletions = current_row[j] + 1
                substitutions = previous_row[j] + (c1 != c2)
                current_row.append(min(insertions, deletions, substitutions))
            previous_row = current_row
        
        return previous_row[-1]

class MultiCityOptimizer:
    """å¤šåŸå¸‚æ‰¹é‡ä¼˜åŒ–å™¨"""
    
    def __init__(self, bus_stops_dir: str, population_dir: str):
        """åˆå§‹åŒ–"""
        self.bus_stops_dir = bus_stops_dir
        self.population_dir = population_dir
        self.coverage_radius = 300  # 300ç±³è¦†ç›–åŠå¾„
        
        # åˆ›å»ºæ‰¹é‡ç»“æœç›®å½•
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.batch_output_dir = Path(f"multi_city_optimization_batch_{timestamp}")
        self.batch_output_dir.mkdir(exist_ok=True)
        
        logger.info(f"ğŸš€ åˆå§‹åŒ–å¤šåŸå¸‚æ‰¹é‡ä¼˜åŒ–å™¨...")
        logger.info(f"ğŸ“ æ‰¹é‡ç»“æœç›®å½•: {self.batch_output_dir}")
        
        # åŸå¸‚æ•°æ®åŒ¹é…å™¨
        self.matcher = CityDataMatcher(bus_stops_dir, population_dir)
        
        # æ‰¹é‡ç»“æœç»Ÿè®¡
        self.batch_results = {
            'total_cities': 0,
            'successful_cities': 0,
            'failed_cities': 0,
            'city_results': [],
            'batch_start_time': datetime.now(),
            'batch_statistics': {}
        }
    
    def run_batch_optimization(self) -> str:
        """è¿è¡Œæ‰¹é‡ä¼˜åŒ–"""
        logger.info("ğŸ¯ å¼€å§‹å¤šåŸå¸‚æ‰¹é‡ä¼˜åŒ–...")
        
        # 1. å‘ç°å¹¶åŒ¹é…åŸå¸‚æ•°æ®
        cities = self.matcher.discover_cities()
        if not cities:
            logger.error("âŒ æœªå‘ç°ä»»ä½•å¯åŒ¹é…çš„åŸå¸‚æ•°æ®ï¼")
            return str(self.batch_output_dir)
        
        self.batch_results['total_cities'] = len(cities)
        
        # 2. æ‰¹é‡ä¼˜åŒ–å¤„ç†
        logger.info(f"ğŸ—ï¸  å¼€å§‹å¤„ç† {len(cities)} ä¸ªåŸå¸‚...")
        
        for i, city_info in enumerate(cities, 1):
            logger.info(f"\n{'='*60}")
            logger.info(f"ğŸ™ï¸  å¤„ç†åŸå¸‚ {i}/{len(cities)}: {city_info['city_name']}")
            logger.info(f"{'='*60}")
            
            try:
                city_result = self._optimize_single_city(city_info, i, len(cities))
                self.batch_results['city_results'].append(city_result)
                
                if city_result['status'] == 'success':
                    self.batch_results['successful_cities'] += 1
                    logger.info(f"âœ… åŸå¸‚ {city_info['city_name']} ä¼˜åŒ–æˆåŠŸï¼")
                else:
                    self.batch_results['failed_cities'] += 1
                    logger.error(f"âŒ åŸå¸‚ {city_info['city_name']} ä¼˜åŒ–å¤±è´¥: {city_result.get('error', 'Unknown')}")
                    
            except Exception as e:
                logger.error(f"âŒ åŸå¸‚ {city_info['city_name']} å¤„ç†å¼‚å¸¸: {e}")
                logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
                
                self.batch_results['city_results'].append({
                    'city_name': city_info['city_name'],
                    'status': 'error',
                    'error': str(e)
                })
                self.batch_results['failed_cities'] += 1
        
        # 3. ç”Ÿæˆæ‰¹é‡ç»“æœæŠ¥å‘Š
        self._create_batch_summary()
        
        # 4. æ˜¾ç¤ºæ‰¹é‡å®Œæˆä¿¡æ¯
        self._display_batch_completion()
        
        return str(self.batch_output_dir)
    
    def _optimize_single_city(self, city_info: Dict, city_index: int, total_cities: int) -> Dict:
        """ä¼˜åŒ–å•ä¸ªåŸå¸‚"""
        city_name = city_info['city_name']
        bus_file = city_info['bus_file']
        pop_file = city_info['population_file']
        
        try:
            # 1. æ•°æ®é¢„å¤„ç†
            logger.info(f"ğŸ“Š {city_name}: å¼€å§‹æ•°æ®é¢„å¤„ç†...")
            start_time = time.time()
            
            processor = DataProcessor(pop_file, bus_file)
            population_data, bus_stops_data, overlap_info = processor.get_processed_data()
            
            preprocessing_time = time.time() - start_time
            
            logger.info(f"âœ… {city_name}: æ•°æ®é¢„å¤„ç†å®Œæˆ ({preprocessing_time:.1f}s)")
            logger.info(f"   äººå£ç½‘æ ¼: {len(population_data):,}")
            logger.info(f"   å…¬äº¤ç«™ç‚¹: {len(bus_stops_data):,}")
            logger.info(f"   æ€»äººå£: {population_data['population'].sum():,.0f}")
            
            # 2. æ•°æ®æœ‰æ•ˆæ€§æ£€æŸ¥
            if len(bus_stops_data) < 10:
                return {
                    'city_name': city_name,
                    'status': 'skipped',
                    'error': f'ç«™ç‚¹æ•°é‡è¿‡å°‘ ({len(bus_stops_data)})',
                    'preprocessing_time': preprocessing_time
                }
            
            if len(population_data) < 50:
                return {
                    'city_name': city_name,
                    'status': 'skipped',
                    'error': f'äººå£ç½‘æ ¼è¿‡å°‘ ({len(population_data)})',
                    'preprocessing_time': preprocessing_time
                }
            
            # 3. æ‰§è¡Œé—ä¼ ç®—æ³•ä¼˜åŒ–
            logger.info(f"âš¡ {city_name}: æ‰§è¡Œé—ä¼ ç®—æ³•ä¼˜åŒ–...")
            
            original_positions = bus_stops_data[['longitude', 'latitude']].values
            pop_points = population_data[['longitude', 'latitude']].values
            pop_weights = population_data['population'].values
            
            optimization_start = time.time()
            
            # æ ¹æ®åŸå¸‚è§„æ¨¡è°ƒæ•´ç®—æ³•å‚æ•°
            population_size = min(60, max(30, len(bus_stops_data) // 100))
            max_generations = min(120, max(50, len(bus_stops_data) // 50))
            
            optimized_positions, best_fitness = adaptive_genetic_algorithm(
                original_positions, pop_points, pop_weights, 
                self.coverage_radius / 111320.0,  # è½¬æ¢ä¸ºåº¦
                population_size=population_size, 
                max_generations=max_generations
            )
            
            optimization_time = time.time() - optimization_start
            
            # 4. è®¡ç®—è¯¦ç»†ç»Ÿè®¡
            results = self._calculate_city_stats(
                city_name, original_positions, optimized_positions, 
                pop_points, pop_weights, preprocessing_time, 
                optimization_time, best_fitness
            )
            
            # 5. ä¿å­˜åŸå¸‚ç»“æœ
            city_output_dir = self.batch_output_dir / f"{city_name}_results"
            city_output_dir.mkdir(exist_ok=True)
            
            self._save_city_results(
                city_output_dir, city_name, population_data, bus_stops_data,
                original_positions, optimized_positions, results
            )
            
            logger.info(f"ğŸ‰ {city_name}: ä¼˜åŒ–å®Œæˆï¼")
            logger.info(f"   ç§»åŠ¨ç«™ç‚¹: {results['moved_stations']}/{results['total_stations']} ({results['moved_stations']/results['total_stations']:.1%})")
            logger.info(f"   è¦†ç›–ç‡: {results['original_coverage']:.3f} â†’ {results['optimized_coverage']:.3f}")
            logger.info(f"   ç”¨æ—¶: é¢„å¤„ç†{preprocessing_time:.1f}s + ä¼˜åŒ–{optimization_time:.1f}s")
            
            results.update({
                'status': 'success',
                'city_name': city_name,
                'output_dir': str(city_output_dir)
            })
            
            return results
            
        except Exception as e:
            logger.error(f"âŒ {city_name}: ä¼˜åŒ–å¤±è´¥ - {e}")
            return {
                'city_name': city_name,
                'status': 'failed',
                'error': str(e)
            }
    
    def _calculate_city_stats(self, city_name, original_positions, optimized_positions, 
                            pop_points, pop_weights, preprocessing_time, 
                            optimization_time, best_fitness):
        """è®¡ç®—åŸå¸‚ä¼˜åŒ–ç»Ÿè®¡"""
        n_stops = len(original_positions)
        
        # è¦†ç›–ç‡è®¡ç®—
        original_coverage = fast_population_coverage(
            original_positions, pop_points, pop_weights, self.coverage_radius / 111320.0
        )
        optimized_coverage = fast_population_coverage(
            optimized_positions, pop_points, pop_weights, self.coverage_radius / 111320.0
        )
        
        # ç§»åŠ¨ç»Ÿè®¡
        movements = []
        moved_count = 0
        total_movement_m = 0.0
        
        for i in range(n_stops):
            dx = optimized_positions[i, 0] - original_positions[i, 0]
            dy = optimized_positions[i, 1] - original_positions[i, 1]
            movement_deg = np.sqrt(dx * dx + dy * dy)
            movement_m = movement_deg * 111320.0
            
            movements.append(movement_m)
            total_movement_m += movement_m
            
            if movement_m > 10.0:  # ç§»åŠ¨è¶…è¿‡10ç±³
                moved_count += 1
        
        stability_score = (n_stops - moved_count) / n_stops
        total_population = pop_weights.sum()
        
        return {
            'city_name': city_name,
            'preprocessing_time': preprocessing_time,
            'optimization_time': optimization_time,
            'total_time': preprocessing_time + optimization_time,
            'best_fitness': best_fitness,
            'total_stations': n_stops,
            'total_population': float(total_population),
            'population_grids': len(pop_points),
            'moved_stations': moved_count,
            'stability_score': stability_score,
            'original_coverage': original_coverage,
            'optimized_coverage': optimized_coverage,
            'coverage_improvement': optimized_coverage - original_coverage,
            'coverage_improvement_pct': (optimized_coverage - original_coverage) / original_coverage * 100 if original_coverage > 0 else 0,
            'total_movement_m': total_movement_m,
            'average_movement_m': total_movement_m / n_stops,
            'movements': movements,
            'original_positions': original_positions,
            'optimized_positions': optimized_positions
        }
    
    def _save_city_results(self, output_dir, city_name, population_data, bus_stops_data,
                          original_positions, optimized_positions, results):
        """ä¿å­˜å•ä¸ªåŸå¸‚çš„ç»“æœ"""
        
        # 1. ä¿å­˜ç«™ç‚¹æ•°æ®
        optimized_stops = bus_stops_data.copy()
        optimized_stops['original_lon'] = original_positions[:, 0]
        optimized_stops['original_lat'] = original_positions[:, 1]
        optimized_stops['longitude'] = optimized_positions[:, 0]
        optimized_stops['latitude'] = optimized_positions[:, 1]
        optimized_stops['movement_m'] = results['movements']
        optimized_stops['is_moved'] = [m > 10.0 for m in results['movements']]
        
        moved_stops = optimized_stops[optimized_stops['is_moved'] == True]
        
        if HAS_GEOPANDAS:
            # ä¿å­˜ä¸ºShapefile
            self._save_city_shapefile(output_dir / "original_bus_stops.shp", bus_stops_data)
            self._save_city_shapefile(output_dir / "optimized_bus_stops.shp", optimized_stops)
            self._save_city_shapefile(output_dir / "moved_bus_stops.shp", moved_stops)
        else:
            # ä¿å­˜ä¸ºCSV
            bus_stops_data.to_csv(output_dir / "original_bus_stops.csv", index=False)
            optimized_stops.to_csv(output_dir / "optimized_bus_stops.csv", index=False)
            moved_stops.to_csv(output_dir / "moved_bus_stops.csv", index=False)
        
        # 2. ä¿å­˜äººå£æ•°æ®
        population_data.to_csv(output_dir / "population_data.csv", index=False)
        
        # 3. ä¿å­˜ç»Ÿè®¡æ•°æ®
        city_stats = {k: v for k, v in results.items() 
                     if k not in ['movements', 'original_positions', 'optimized_positions']}
        
        with open(output_dir / f"{city_name}_optimization_stats.json", 'w', encoding='utf-8') as f:
            json.dump(city_stats, f, indent=2, ensure_ascii=False)
        
        # 4. ç”ŸæˆåŸå¸‚æŠ¥å‘Š
        self._create_city_report(output_dir, city_name, results)
    
    def _save_city_shapefile(self, filename, data):
        """ä¿å­˜åŸå¸‚shapefile"""
        if HAS_GEOPANDAS:
            gdf = gpd.GeoDataFrame(
                data,
                geometry=gpd.points_from_xy(data['longitude'], data['latitude']),
                crs='EPSG:4326'
            )
            gdf.to_file(filename, encoding='utf-8')
    
    def _create_city_report(self, output_dir, city_name, results):
        """åˆ›å»ºåŸå¸‚ä¼˜åŒ–æŠ¥å‘Š"""
        report_path = output_dir / f"{city_name}_optimization_report.txt"
        movements = np.array(results['movements'])
        moved_movements = movements[movements > 10.0]
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(f"=== {city_name} å…¬äº¤ç«™ç‚¹ä¼˜åŒ–æŠ¥å‘Š ===\n\n")
            f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"ç®—æ³•ç‰ˆæœ¬: final_optimizer_with_outputs.py (å¤šåŸå¸‚æ‰¹é‡ç‰ˆ)\n\n")
            
            f.write("== ä¼˜åŒ–å‚æ•° ==\n")
            f.write(f"è¦†ç›–åŠå¾„: {self.coverage_radius}ç±³\n")
            f.write(f"æ•°æ®é¢„å¤„ç†æ—¶é—´: {results['preprocessing_time']:.2f}ç§’\n")
            f.write(f"ç®—æ³•ä¼˜åŒ–æ—¶é—´: {results['optimization_time']:.2f}ç§’\n")
            f.write(f"æ€»å¤„ç†æ—¶é—´: {results['total_time']:.2f}ç§’\n\n")
            
            f.write("== æ•°æ®è§„æ¨¡ ==\n")
            f.write(f"äººå£ç½‘æ ¼æ•°: {results['population_grids']:,}\n")
            f.write(f"æ€»æœåŠ¡äººå£: {results['total_population']:,.0f}\n")
            f.write(f"å…¬äº¤ç«™ç‚¹æ•°: {results['total_stations']:,}\n\n")
            
            f.write("== ä¼˜åŒ–ç»“æœ ==\n")
            f.write(f"ç§»åŠ¨ç«™ç‚¹æ•°: {results['moved_stations']:,} ({results['moved_stations']/results['total_stations']:.2%})\n")
            f.write(f"ç¨³å®šæ€§å¾—åˆ†: {results['stability_score']:.4f}\n")
            f.write(f"åŸå§‹è¦†ç›–ç‡: {results['original_coverage']:.4f}\n")
            f.write(f"ä¼˜åŒ–åè¦†ç›–ç‡: {results['optimized_coverage']:.4f}\n")
            f.write(f"è¦†ç›–ç‡æå‡: {results['coverage_improvement']:.4f} ({results['coverage_improvement_pct']:.1f}%)\n")
            f.write(f"å¹³å‡ç§»åŠ¨è·ç¦»: {results['average_movement_m']:.2f}ç±³\n")
            
            if len(moved_movements) > 0:
                f.write(f"ç§»åŠ¨ç«™ç‚¹å¹³å‡è·ç¦»: {np.mean(moved_movements):.2f}ç±³\n")
                f.write(f"æœ€å¤§ç§»åŠ¨è·ç¦»: {np.max(moved_movements):.2f}ç±³\n")
            
            f.write(f"æ€»ç§»åŠ¨è·ç¦»: {results['total_movement_m']:.2f}ç±³\n\n")
            
            f.write("æŠ¥å‘Šå®Œæˆã€‚\n")
    
    def _create_batch_summary(self):
        """åˆ›å»ºæ‰¹é‡ä¼˜åŒ–æ±‡æ€»æŠ¥å‘Š"""
        logger.info("ğŸ“‹ ç”Ÿæˆæ‰¹é‡ä¼˜åŒ–æ±‡æ€»æŠ¥å‘Š...")
        
        # 1. è®¡ç®—æ‰¹é‡ç»Ÿè®¡
        successful_results = [r for r in self.batch_results['city_results'] if r.get('status') == 'success']
        
        if successful_results:
            # æ±‡æ€»ç»Ÿè®¡
            total_stations = sum(r.get('total_stations', 0) for r in successful_results)
            total_moved = sum(r.get('moved_stations', 0) for r in successful_results)
            total_population = sum(r.get('total_population', 0) for r in successful_results)
            avg_coverage_improvement = np.mean([r.get('coverage_improvement_pct', 0) for r in successful_results])
            total_optimization_time = sum(r.get('optimization_time', 0) for r in successful_results)
            
            self.batch_results['batch_statistics'] = {
                'total_stations_processed': total_stations,
                'total_stations_moved': total_moved,
                'overall_move_percentage': total_moved / total_stations if total_stations > 0 else 0,
                'total_population_served': total_population,
                'average_coverage_improvement_pct': avg_coverage_improvement,
                'total_optimization_time': total_optimization_time,
                'average_time_per_city': total_optimization_time / len(successful_results) if successful_results else 0
            }
        
        # 2. ä¿å­˜æ±‡æ€»ç»Ÿè®¡JSON
        batch_stats_file = self.batch_output_dir / "batch_optimization_summary.json"
        with open(batch_stats_file, 'w', encoding='utf-8') as f:
            # è½¬æ¢numpyç±»å‹ä¸ºPythonåŸç”Ÿç±»å‹
            serializable_results = []
            for result in self.batch_results['city_results']:
                clean_result = {}
                for k, v in result.items():
                    if k in ['movements', 'original_positions', 'optimized_positions']:
                        continue  # è·³è¿‡å¤§æ•°ç»„
                    elif isinstance(v, (np.integer, np.floating)):
                        clean_result[k] = float(v)
                    elif isinstance(v, np.ndarray):
                        clean_result[k] = v.tolist()
                    else:
                        clean_result[k] = v
                serializable_results.append(clean_result)
            
            summary_data = {
                'batch_info': {
                    'start_time': self.batch_results['batch_start_time'].isoformat(),
                    'end_time': datetime.now().isoformat(),
                    'total_cities': self.batch_results['total_cities'],
                    'successful_cities': self.batch_results['successful_cities'],
                    'failed_cities': self.batch_results['failed_cities'],
                    'success_rate': self.batch_results['successful_cities'] / self.batch_results['total_cities'] if self.batch_results['total_cities'] > 0 else 0
                },
                'batch_statistics': self.batch_results.get('batch_statistics', {}),
                'city_results': serializable_results
            }
            
            json.dump(summary_data, f, indent=2, ensure_ascii=False)
        
        # 3. ç”Ÿæˆæ–‡æœ¬æ±‡æ€»æŠ¥å‘Š
        self._create_batch_text_report()
        
        logger.info(f"ğŸ“Š æ‰¹é‡æ±‡æ€»æŠ¥å‘Šå·²ä¿å­˜: {batch_stats_file}")
    
    def _create_batch_text_report(self):
        """ç”Ÿæˆæ‰¹é‡ä¼˜åŒ–æ–‡æœ¬æŠ¥å‘Š"""
        report_path = self.batch_output_dir / "batch_optimization_report.txt"
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("=== å¤šåŸå¸‚å…¬äº¤ç«™ç‚¹æ‰¹é‡ä¼˜åŒ–æŠ¥å‘Š ===\n\n")
            f.write(f"æ‰¹é‡å¤„ç†æ—¶é—´: {self.batch_results['batch_start_time'].strftime('%Y-%m-%d %H:%M:%S')} - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"ç®—æ³•ç‰ˆæœ¬: final_optimizer_with_outputs.py (UltraThink å¤šåŸå¸‚æ‰¹é‡ç‰ˆ)\n\n")
            
            f.write("== æ‰¹é‡å¤„ç†æ¦‚å†µ ==\n")
            f.write(f"æ€»å¤„ç†åŸå¸‚æ•°: {self.batch_results['total_cities']}\n")
            f.write(f"æˆåŠŸä¼˜åŒ–åŸå¸‚: {self.batch_results['successful_cities']}\n")
            f.write(f"å¤±è´¥åŸå¸‚æ•°: {self.batch_results['failed_cities']}\n")
            f.write(f"æˆåŠŸç‡: {self.batch_results['successful_cities']/self.batch_results['total_cities']:.1%}\n\n")
            
            if 'batch_statistics' in self.batch_results:
                stats = self.batch_results['batch_statistics']
                f.write("== æ‰¹é‡ä¼˜åŒ–ç»Ÿè®¡ ==\n")
                f.write(f"æ€»å¤„ç†ç«™ç‚¹æ•°: {stats.get('total_stations_processed', 0):,}\n")
                f.write(f"æ€»ç§»åŠ¨ç«™ç‚¹æ•°: {stats.get('total_stations_moved', 0):,}\n")
                f.write(f"æ•´ä½“ç§»åŠ¨æ¯”ä¾‹: {stats.get('overall_move_percentage', 0):.2%}\n")
                f.write(f"æ€»æœåŠ¡äººå£: {stats.get('total_population_served', 0):,.0f}\n")
                f.write(f"å¹³å‡è¦†ç›–ç‡æå‡: {stats.get('average_coverage_improvement_pct', 0):.1f}%\n")
                f.write(f"æ€»ä¼˜åŒ–æ—¶é—´: {stats.get('total_optimization_time', 0):.1f}ç§’\n")
                f.write(f"å¹³å‡æ¯åŸå¸‚ç”¨æ—¶: {stats.get('average_time_per_city', 0):.1f}ç§’\n\n")
            
            f.write("== å„åŸå¸‚ä¼˜åŒ–è¯¦æƒ… ==\n")
            for i, result in enumerate(self.batch_results['city_results'], 1):
                city_name = result.get('city_name', f'City_{i}')
                status = result.get('status', 'unknown')
                
                f.write(f"{i}. {city_name}: {status.upper()}\n")
                
                if status == 'success':
                    f.write(f"   ç«™ç‚¹æ•°: {result.get('total_stations', 0):,} (ç§»åŠ¨: {result.get('moved_stations', 0):,})\n")
                    f.write(f"   è¦†ç›–ç‡æå‡: {result.get('coverage_improvement_pct', 0):.1f}%\n")
                    f.write(f"   ç”¨æ—¶: {result.get('total_time', 0):.1f}ç§’\n")
                elif status in ['failed', 'error']:
                    f.write(f"   é”™è¯¯: {result.get('error', 'Unknown error')}\n")
                elif status == 'skipped':
                    f.write(f"   è·³è¿‡åŸå› : {result.get('error', 'Unknown reason')}\n")
                
                f.write("\n")
            
            f.write("== ç®—æ³•ç‰¹ç‚¹ ==\n")
            f.write("1. åŸºäºfinal_optimizer_with_outputs.pyçš„æˆç†Ÿç®—æ³•\n")
            f.write("2. æ™ºèƒ½åŸå¸‚æ•°æ®åŒ¹é…å’Œé¢„å¤„ç†\n")
            f.write("3. è‡ªé€‚åº”å‚æ•°è°ƒæ•´é€‚åº”ä¸åŒè§„æ¨¡åŸå¸‚\n")
            f.write("4. ç¨³å®šæ€§ä¼˜å…ˆçš„ä¿å®ˆä¼˜åŒ–ç­–ç•¥\n")
            f.write("5. å®Œæ•´çš„ç»“æœæ–‡ä»¶è¾“å‡ºå’Œç»Ÿè®¡æŠ¥å‘Š\n\n")
            
            f.write("æ‰¹é‡ä¼˜åŒ–æŠ¥å‘Šå®Œæˆã€‚\n")
    
    def _display_batch_completion(self):
        """æ˜¾ç¤ºæ‰¹é‡å®Œæˆä¿¡æ¯"""
        logger.info("\n" + "="*80)
        logger.info("ğŸ‰ å¤šåŸå¸‚æ‰¹é‡ä¼˜åŒ–å®Œæˆï¼")
        logger.info("="*80)
        logger.info(f"ğŸ“Š å¤„ç†ç»“æœ:")
        logger.info(f"   æ€»åŸå¸‚æ•°: {self.batch_results['total_cities']}")
        logger.info(f"   æˆåŠŸä¼˜åŒ–: {self.batch_results['successful_cities']}")
        logger.info(f"   å¤±è´¥æ•°é‡: {self.batch_results['failed_cities']}")
        logger.info(f"   æˆåŠŸç‡: {self.batch_results['successful_cities']/self.batch_results['total_cities']:.1%}")
        
        if 'batch_statistics' in self.batch_results:
            stats = self.batch_results['batch_statistics']
            logger.info(f"ğŸšŒ ä¼˜åŒ–ç»Ÿè®¡:")
            logger.info(f"   æ€»ç«™ç‚¹: {stats.get('total_stations_processed', 0):,}")
            logger.info(f"   ç§»åŠ¨ç«™ç‚¹: {stats.get('total_stations_moved', 0):,}")
            logger.info(f"   å¹³å‡è¦†ç›–æå‡: {stats.get('average_coverage_improvement_pct', 0):.1f}%")
        
        logger.info(f"ğŸ“ ç»“æœä¿å­˜ä½ç½®: {self.batch_output_dir}")
        logger.info("="*80 + "\n")

def main():
    """ä¸»å‡½æ•°"""
    logger.info("ğŸš€ å¯åŠ¨å¤šåŸå¸‚æ‰¹é‡ä¼˜åŒ–ç³»ç»Ÿ (UltraThinkç‰ˆ)...")
    logger.info("ğŸ¯ åŸºäºfinal_optimizer_with_outputs.pyçš„æˆç†Ÿç®—æ³•")
    
    # é…ç½®è·¯å¾„
    bus_stops_dir = "./å…¬äº¤ç«™ç‚¹shp"
    population_dir = "./populaiton"
    
    # æ£€æŸ¥ç›®å½•æ˜¯å¦å­˜åœ¨
    if not Path(bus_stops_dir).exists():
        logger.error(f"âŒ å…¬äº¤ç«™ç‚¹ç›®å½•ä¸å­˜åœ¨: {bus_stops_dir}")
        return
    
    if not Path(population_dir).exists():
        logger.error(f"âŒ äººå£æ•°æ®ç›®å½•ä¸å­˜åœ¨: {population_dir}")
        return
    
    # åˆ›å»ºå¹¶è¿è¡Œæ‰¹é‡ä¼˜åŒ–å™¨
    optimizer = MultiCityOptimizer(bus_stops_dir, population_dir)
    result_dir = optimizer.run_batch_optimization()
    
    logger.info(f"ğŸŠ å¤šåŸå¸‚æ‰¹é‡ä¼˜åŒ–å¤§åŠŸå‘Šæˆï¼")
    logger.info(f"ğŸ“ å®Œæ•´ç»“æœæŸ¥çœ‹: {result_dir}")
    logger.info("ğŸ’¡ æ¯ä¸ªåŸå¸‚éƒ½æœ‰ç‹¬ç«‹çš„ç»“æœæ–‡ä»¶å¤¹å’Œå®Œæ•´åˆ†ææŠ¥å‘Š")

if __name__ == "__main__":
    main()