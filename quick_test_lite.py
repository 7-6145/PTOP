"""
å¿«é€Ÿæµ‹è¯•è„šæœ¬ - è½»é‡çº§ç‰ˆæœ¬
åªæµ‹è¯•å‰100ä¸ªç«™ç‚¹ï¼Œç”¨äºå¿«é€ŸéªŒè¯ç³»ç»ŸåŠŸèƒ½
é¢„è®¡è¿è¡Œæ—¶é—´ï¼š3-5åˆ†é’Ÿ
"""

import numpy as np
import pandas as pd
import logging
import time
from typing import Dict, Any

from data_preprocessing import DataProcessor
from spatial_index import SpatialIndex, PopulationCoverageCalculator
from reward_function import RewardFunction
from bus_optimization_env import BusStopOptimizationEnv

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class QuickOptimizer:
    """
    å¿«é€Ÿä¼˜åŒ–å™¨ - ç”¨äºå¿«é€Ÿæµ‹è¯•å’Œæ¼”ç¤º
    ä½¿ç”¨ç®€åŒ–çš„å¯å‘å¼ç®—æ³•æ›¿ä»£å¤æ‚çš„PPOè®­ç»ƒ
    """
    
    def __init__(self, population_csv_path: str, bus_stops_shp_path: str, max_stations: int = 100):
        """
        åˆå§‹åŒ–å¿«é€Ÿä¼˜åŒ–å™¨
        
        Args:
            population_csv_path: äººå£æ•°æ®è·¯å¾„
            bus_stops_shp_path: å…¬äº¤ç«™ç‚¹æ•°æ®è·¯å¾„
            max_stations: æœ€å¤§æµ‹è¯•ç«™ç‚¹æ•°ï¼ˆé»˜è®¤100ï¼‰
        """
        self.population_csv_path = population_csv_path
        self.bus_stops_shp_path = bus_stops_shp_path
        
        # åŠ è½½æ•°æ®
        processor = DataProcessor(population_csv_path, bus_stops_shp_path)
        self.population_data, bus_stops_data, _ = processor.get_processed_data()
        
        # ğŸ”¥ å…³é”®ä¿®æ”¹ï¼šåªä½¿ç”¨å‰Nä¸ªç«™ç‚¹è¿›è¡Œæµ‹è¯•
        self.bus_stops_data = bus_stops_data.iloc[:max_stations].copy()
        logger.info(f"âš¡ è½»é‡çº§æ¨¡å¼ï¼šåªä½¿ç”¨å‰ {len(self.bus_stops_data)} ä¸ªç«™ç‚¹è¿›è¡Œæµ‹è¯•")
        
        # åˆå§‹åŒ–å¥–åŠ±å‡½æ•°ï¼ˆæé«˜è¦†ç›–ç‡æƒé‡ï¼Œè®©å¾®å°æå‡ä¹Ÿæœ‰ä»·å€¼ï¼‰
        self.reward_function = RewardFunction(
            self.population_data, 
            self.bus_stops_data, 
            coverage_radius=0.01,
            coverage_weight=100.0,  # æé«˜100å€ï¼Œè®©0.01%çš„æå‡ä¹Ÿèƒ½æŠµæ¶ˆç§»åŠ¨æƒ©ç½š
            movement_weight=0.05,
            stability_weight=0.01
        )
        
        # åŸå§‹ç«™ç‚¹ä½ç½®
        self.original_positions = self.bus_stops_data[['longitude', 'latitude']].values
        
        logger.info(f"å¿«é€Ÿä¼˜åŒ–å™¨åˆå§‹åŒ–å®Œæˆï¼Œ{len(self.original_positions)} ä¸ªç«™ç‚¹")
    
    def gradient_based_optimization(self, max_iterations: int = 50, 
                                   step_size: float = 0.0001) -> Dict[str, Any]:
        """
        åŸºäºæ¢¯åº¦çš„å¿«é€Ÿä¼˜åŒ–ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
        
        Args:
            max_iterations: æœ€å¤§è¿­ä»£æ¬¡æ•°
            step_size: æ­¥é•¿
            
        Returns:
            ä¼˜åŒ–ç»“æœ
        """
        logger.info("å¼€å§‹åŸºäºæ¢¯åº¦çš„å¿«é€Ÿä¼˜åŒ–...")
        start_time = time.time()
        
        current_positions = self.original_positions.copy()
        best_positions = current_positions.copy()
        
        # è®¡ç®—åˆå§‹å¥–åŠ±
        initial_reward = self.reward_function.calculate_reward(current_positions)
        best_reward = initial_reward['total_reward']
        
        logger.info(f"åˆå§‹å¥–åŠ±: {best_reward:.6f}")
        logger.info(f"åˆå§‹è¦†ç›–ç‡: {initial_reward['coverage_ratio']:.4f}")
        
        history = []
        
        for iteration in range(max_iterations):
            improved = False
            
            # å¯¹æ¯ä¸ªç«™ç‚¹å°è¯•å°å¹…ç§»åŠ¨
            for i in range(len(current_positions)):
                original_pos = current_positions[i].copy()
                
                # å°è¯•8ä¸ªæ–¹å‘çš„ç§»åŠ¨
                directions = [
                    [step_size, 0], [-step_size, 0], [0, step_size], [0, -step_size],
                    [step_size, step_size], [step_size, -step_size], 
                    [-step_size, step_size], [-step_size, -step_size]
                ]
                
                best_move = None
                best_local_reward = best_reward
                
                for direction in directions:
                    # å°è¯•ç§»åŠ¨
                    test_positions = current_positions.copy()
                    test_positions[i] = original_pos + np.array(direction)
                    
                    # æ£€æŸ¥è¾¹ç•Œ
                    bounds_lower, bounds_upper = self.reward_function.get_optimization_bounds()
                    if (test_positions[i, 0] < bounds_lower[i, 0] or 
                        test_positions[i, 0] > bounds_upper[i, 0] or
                        test_positions[i, 1] < bounds_lower[i, 1] or 
                        test_positions[i, 1] > bounds_upper[i, 1]):
                        continue
                    
                    # è®¡ç®—å¥–åŠ±
                    reward_dict = self.reward_function.calculate_reward(test_positions)
                    test_reward = reward_dict['total_reward']
                    
                    if test_reward > best_local_reward:
                        best_local_reward = test_reward
                        best_move = direction
                        improved = True
                
                # åº”ç”¨æœ€ä½³ç§»åŠ¨
                if best_move is not None:
                    current_positions[i] = original_pos + np.array(best_move)
                    best_reward = best_local_reward
                    best_positions = current_positions.copy()
            
            # è®°å½•å†å²
            current_reward_dict = self.reward_function.calculate_reward(current_positions)
            history.append({
                'iteration': iteration + 1,
                'reward': current_reward_dict['total_reward'],
                'coverage_ratio': current_reward_dict['coverage_ratio'],
                'coverage_improvement': current_reward_dict['coverage_improvement'],
                'total_movement': current_reward_dict['total_movement']
            })
            
            # æ‰“å°è¿›åº¦ï¼ˆæ¯5æ¬¡è¿­ä»£ï¼‰
            if (iteration + 1) % 5 == 0:
                logger.info(f"è¿­ä»£ {iteration + 1}: å¥–åŠ± {best_reward:.6f}, "
                          f"è¦†ç›–ç‡ {current_reward_dict['coverage_ratio']:.4f}")
            
            # æ£€æŸ¥æ”¶æ•›
            if not improved:
                logger.info(f"ç®—æ³•æ”¶æ•›äºè¿­ä»£ {iteration + 1}")
                break
        
        optimization_time = time.time() - start_time
        
        # æœ€ç»ˆè´¨é‡è¯„ä¼°
        final_quality = self.reward_function.evaluate_solution_quality(best_positions)
        
        result = {
            'method': 'gradient_based',
            'best_positions': best_positions,
            'original_positions': self.original_positions,
            'best_reward': best_reward,
            'initial_reward': initial_reward['total_reward'],
            'improvement': best_reward - initial_reward['total_reward'],
            'optimization_time': optimization_time,
            'iterations': len(history),
            'quality_report': final_quality,
            'history': history
        }
        
        logger.info(f"å¿«é€Ÿä¼˜åŒ–å®Œæˆ! ç”¨æ—¶: {optimization_time:.2f}ç§’")
        logger.info(f"å¥–åŠ±æ”¹å–„: {result['improvement']:.6f}")
        logger.info(f"è¦†ç›–ç‡æ”¹å–„: {final_quality['quality_metrics']['coverage_improvement_percent']:.2f}%")
        
        return result
    
    def random_search_optimization(self, max_iterations: int = 100, 
                                  search_radius: float = 0.002) -> Dict[str, Any]:
        """
        éšæœºæœç´¢ä¼˜åŒ–ï¼ˆæ›´å¿«çš„åŸºçº¿æ–¹æ³•ï¼‰
        
        Args:
            max_iterations: æœ€å¤§è¿­ä»£æ¬¡æ•°
            search_radius: æœç´¢åŠå¾„
            
        Returns:
            ä¼˜åŒ–ç»“æœ
        """
        logger.info("å¼€å§‹éšæœºæœç´¢ä¼˜åŒ–...")
        start_time = time.time()
        
        best_positions = self.original_positions.copy()
        initial_reward = self.reward_function.calculate_reward(best_positions)
        best_reward = initial_reward['total_reward']
        
        history = []
        
        for iteration in range(max_iterations):
            # ç”Ÿæˆéšæœºæ‰°åŠ¨
            n_stations_to_move = max(1, len(self.original_positions) // 4)  # ç§»åŠ¨25%çš„ç«™ç‚¹
            stations_to_move = np.random.choice(len(self.original_positions), 
                                              n_stations_to_move, replace=False)
            
            test_positions = best_positions.copy()
            
            for station_idx in stations_to_move:
                # éšæœºç§»åŠ¨
                perturbation = np.random.normal(0, search_radius, 2)
                test_positions[station_idx] += perturbation
            
            # æ£€æŸ¥è¾¹ç•Œ
            bounds_lower, bounds_upper = self.reward_function.get_optimization_bounds()
            test_positions = np.clip(test_positions, bounds_lower, bounds_upper)
            
            # è®¡ç®—å¥–åŠ±
            reward_dict = self.reward_function.calculate_reward(test_positions)
            test_reward = reward_dict['total_reward']
            
            # æ›´æ–°æœ€ä½³è§£
            if test_reward > best_reward:
                best_positions = test_positions.copy()
                best_reward = test_reward
                logger.info(f"è¿­ä»£ {iteration + 1}: å‘ç°æ›´ä¼˜è§£ï¼Œå¥–åŠ± {best_reward:.6f}")
            
            # è®°å½•å†å²ï¼ˆæ¯10æ¬¡è¿­ä»£ï¼‰
            if (iteration + 1) % 10 == 0:
                current_quality = self.reward_function.calculate_reward(best_positions)
                history.append({
                    'iteration': iteration + 1,
                    'reward': best_reward,
                    'coverage_ratio': current_quality['coverage_ratio'],
                    'coverage_improvement': current_quality['coverage_improvement']
                })
        
        optimization_time = time.time() - start_time
        final_quality = self.reward_function.evaluate_solution_quality(best_positions)
        
        result = {
            'method': 'random_search',
            'best_positions': best_positions,
            'original_positions': self.original_positions,
            'best_reward': best_reward,
            'initial_reward': initial_reward['total_reward'],
            'improvement': best_reward - initial_reward['total_reward'],
            'optimization_time': optimization_time,
            'iterations': max_iterations,
            'quality_report': final_quality,
            'history': history
        }
        
        logger.info(f"éšæœºæœç´¢å®Œæˆ! ç”¨æ—¶: {optimization_time:.2f}ç§’")
        logger.info(f"å¥–åŠ±æ”¹å–„: {result['improvement']:.6f}")
        
        return result


def test_all_modules_quickly():
    """å¿«é€Ÿæµ‹è¯•æ‰€æœ‰æ¨¡å— - è½»é‡çº§ç‰ˆæœ¬"""
    logger.info("=== å¼€å§‹å¿«é€Ÿæ¨¡å—æµ‹è¯•ï¼ˆè½»é‡çº§ï¼‰ ===")
    
    try:
        # 1. æ•°æ®é¢„å¤„ç†æµ‹è¯•
        logger.info("\n1. æµ‹è¯•æ•°æ®é¢„å¤„ç†...")
        processor = DataProcessor(
            #"./populaiton/æ¸©å·_population_grid.csv",
            "D:\Academic\Task138_Bus\Population\London_pop.csv",
            #"./å…¬äº¤ç«™ç‚¹shp/0577æ¸©å·.shp"
            "D:\Academic\Task138_Bus\BUS\London.shp"
        )
        pop_data, bus_data, analysis = processor.get_processed_data()
        # åªä½¿ç”¨å‰100ä¸ªç«™ç‚¹
        bus_data = bus_data.iloc[:100].copy()
        logger.info(f"âœ“ æ•°æ®åŠ è½½æˆåŠŸ: {len(pop_data)} äººå£ç‚¹, {len(bus_data)} ç«™ç‚¹ï¼ˆæµ‹è¯•ç‰ˆï¼‰")
        
        # 2. ç©ºé—´ç´¢å¼•æµ‹è¯•
        logger.info("\n2. æµ‹è¯•ç©ºé—´ç´¢å¼•...")
        coverage_calc = PopulationCoverageCalculator(pop_data, coverage_radius=0.01)
        test_positions = bus_data[['longitude', 'latitude']].values[:5]  # åªæµ‹è¯•å‰5ä¸ªç«™ç‚¹
        coverage = coverage_calc.calculate_coverage_ratio_fast(test_positions)
        logger.info(f"âœ“ ç©ºé—´ç´¢å¼•æ­£å¸¸ï¼Œæµ‹è¯•è¦†ç›–ç‡: {coverage:.4f}")
        
        # 3. å¥–åŠ±å‡½æ•°æµ‹è¯•
        logger.info("\n3. æµ‹è¯•å¥–åŠ±å‡½æ•°...")
        reward_func = RewardFunction(pop_data, bus_data, coverage_radius=0.01)
        original_positions = bus_data[['longitude', 'latitude']].values
        reward_dict = reward_func.calculate_reward(original_positions)
        logger.info(f"âœ“ å¥–åŠ±å‡½æ•°æ­£å¸¸ï¼ŒåŸºçº¿å¥–åŠ±: {reward_dict['total_reward']:.6f}")
        
        # 4. ç¯å¢ƒæµ‹è¯•ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
        logger.info("\n4. æµ‹è¯•å¼ºåŒ–å­¦ä¹ ç¯å¢ƒ...")
        env = BusStopOptimizationEnv(
            "./populaiton/æ¸©å·_population_grid.csv",
            "./å…¬äº¤ç«™ç‚¹shp/0577æ¸©å·.shp",
            max_episode_steps=5  # åªæµ‹è¯•5æ­¥
        )
        obs, info = env.reset()
        action = env.action_space.sample()
        obs, reward, terminated, truncated, info = env.step(action)
        logger.info(f"âœ“ ç¯å¢ƒæµ‹è¯•æ­£å¸¸ï¼Œæµ‹è¯•å¥–åŠ±: {reward:.6f}")
        env.close()
        
        # 5. å¿«é€Ÿä¼˜åŒ–æµ‹è¯•ï¼ˆè½»é‡çº§ï¼‰
        logger.info("\n5. æµ‹è¯•å¿«é€Ÿä¼˜åŒ–ç®—æ³•ï¼ˆè½»é‡çº§ï¼‰...")
        quick_optimizer = QuickOptimizer(
            "./populaiton/æ¸©å·_population_grid.csv",
            "./å…¬äº¤ç«™ç‚¹shp/0577æ¸©å·.shp",
            max_stations=1000
        )
        
        # æ¢¯åº¦ä¼˜åŒ–æµ‹è¯•ï¼ˆå‡å°‘è¿­ä»£æ¬¡æ•°ï¼Œå¢å¤§æ­¥é•¿ï¼‰
        result1 = quick_optimizer.gradient_based_optimization(max_iterations=10, step_size=0.001)
        logger.info(f"âœ“ æ¢¯åº¦ä¼˜åŒ–å®Œæˆï¼Œæ”¹å–„: {result1['improvement']:.6f}")
        
        # éšæœºæœç´¢æµ‹è¯•ï¼ˆå‡å°‘è¿­ä»£æ¬¡æ•°ï¼‰
        result2 = quick_optimizer.random_search_optimization(max_iterations=30)
        logger.info(f"âœ“ éšæœºæœç´¢å®Œæˆï¼Œæ”¹å–„: {result2['improvement']:.6f}")
        
        logger.info("\n=== æ‰€æœ‰æ¨¡å—æµ‹è¯•é€šè¿‡! ===")
        logger.info("âœ¨ è½»é‡çº§æµ‹è¯•å®Œæˆï¼Œç³»ç»Ÿè¿è¡Œæ­£å¸¸")
        logger.info("ğŸ’¡ å¦‚éœ€å®Œæ•´æµ‹è¯•ï¼Œè¯·è¿è¡Œ quick_test.py")
        
        return {
            'data_test': True,
            'spatial_index_test': True,
            'reward_function_test': True,
            'environment_test': True,
            'gradient_optimization_result': result1,
            'random_search_result': result2
        }
        
    except Exception as e:
        logger.error(f"æ¨¡å—æµ‹è¯•å¤±è´¥: {e}")
        raise


if __name__ == "__main__":
    test_all_modules_quickly()

